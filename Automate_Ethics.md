# Can Ethics be Automated?

According to the Merriam-Webster dictionary, ethics are “the principles of conduct governing an individual or a group.” By this definition, I see no reason that a machine or machines can not be the ones being governed. Ethics when perceived as a set of rules is exactly the sort of thing that automation excels at; a machine can follow the same set of rules indefinitely, without hesitation or variation. The consequence of that faithful approach is lack of flexibility. While a human can choose to take context into account, a machine would only be able to do so, if explicitly programed to. The author argues that the ethical decisions made through automation should be able to be overwritten, but as those decisions grow in volume, the question of who decides becomes harder along with the human tendency to not take the action to change decisions that are presented to us. For example, if an algorithm is suggesting prison sentences to a busy judge, more often then not, he will likely follow the recommendation. If he is taking the time to consider every recommendation, then it defeats the purpose of the tool. While, yes, sometimes he will decide to override the algorithm, I doubt it will be often.

Ethics has been argued since probably the dawn of civilization. The age of artificial intelligence will force it into the forefront of society. In an enormously complicated system like Google’s Gmail many programmers over many years contributed to the algorithm it uses in the spam/not spam decision. While email is a low-stakes arena, these will be built for more life-altering situations and who will be liable when a bad outcome accrues will be highly debated. Is it the company that wrote the algorithm? Will the developers themselves be questions why they wrote a line of code one way versus another? Or in the case of one company licensing another’s technology, which company will be at fault? 

While I do not have a comprehensive solution, I do think that a new field needs to be created for “Ethical Auditing.” Companies are forced by law to submit to regular independent finical audits. I believe this model needs to be emulated by firms consisting of philosophers, ethicists, lawyers, and programmers, who under non-disclosure agreements ask about the assumptions used by a company’s ai developers and run the ai itself through simulations designed to put it to the test.

Without a system of independent review, it will be difficult for a programmer to builds a system’s “principles of conduct” outside of his or her own ideologies. For example, Facebook has been accused of using its algorithms to hide certain content they find offensive, despite the argument for free speech. Can Facebook block content from their site? Legally, they may be able to, but is it moral to do so? What happens when one group’s morality is misaligned with another’s?

Looking forward 5 years, I think a lot of these questions will start to get answers as new laws are passed. I do think that there will be a period of turmoil and debate as the new situations arise and arguments over the moral obligations of technology developers are had. The beginnings of these are already there, with Senator Elizabeth Warren as a vocal voice against technology companies. Father out, at 10 years, I think debates will have moved from the moral obligations of automated ethics to the morality of allowing automation to take human jobs. Historically, each time technology has sufficiently advanced, human jobs were displaced and humans were forced to move to work that was harder to automate. However, with the rapid advancement of ai in every area imaginable over recent years, the list jobs that are unable to be automated is getting shorter all the time. If as many experts suspect, this begins to force up the unemployment rate, I expect this will become the new battleground of moral obligations of technology for humanity.
