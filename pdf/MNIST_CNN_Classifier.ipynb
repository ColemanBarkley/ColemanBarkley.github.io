{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Supporting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize Weights in the Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a tf.Variable used to store weights in a filter\n",
    "This variable is initialized with values that can be used to initialize the weights\n",
    "They are random numbers\n",
    "\"\"\"\n",
    "def initialize_weights(filter_shape):\n",
    "    init_random_dist = tf.truncated_normal(filter_shape, stddev=0.1)\n",
    "    return (tf.Variable(init_random_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a tf.Variable used to store bias\n",
    "This variable is initialized with values that can be used to initialize the bias\n",
    "They are random numbers\n",
    "\"\"\"\n",
    "def initialize_bias(bias_shape):\n",
    "    initalize_bias_vals = tf.constant(0.1, shape=bias_shape)\n",
    "    return (tf.Variable(initalize_bias_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Conv Layer and Perform Conv Computation: Dot Product (x * W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a function to set up a conv layer (conv2d)\n",
    "Paramaters: \n",
    "    inputs: [batch, H, W, Channels]\n",
    "    filter_shape:\n",
    "    [filter H, filter W, in_channels(in_depth = in_num_filters), out_channels(out_depth = out_num_filters)]\n",
    "    Return: Outputs of the Layer: the dot product: inputs * weights): x * W\n",
    "\"\"\"\n",
    "\n",
    "def create_convolutional_layer_and_compute_dot_product(inputs, filter_shape):\n",
    "    # Initialize the weights in the filter\n",
    "    filter_initialized_with_weights = initialize_weights(filter_shape)\n",
    "    \n",
    "    # Create Conv layer\n",
    "    conv_layer_outputs = tf.nn.conv2d(inputs, filter_initialized_with_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    # Return the Conv Layer Outputs\n",
    "    return (conv_layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a ReLU Layer and perform Computation: Dot Product + Bias (x.W + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the function\n",
    "Setup ReLU layer\n",
    "Perform Computation\n",
    "\"\"\"\n",
    "\n",
    "def create_relu_layer_and_compute_dotproduct_plus_b(inputs, filter_shape):\n",
    "    # Intialize biad for each input channel\n",
    "    b = initialize_bias([filter_shape[3]])\n",
    "    \n",
    "    # Perform the computation by first adding the inputs\n",
    "    # Then create a ReLU layer associated with this Conv Layer\n",
    "    relu_layer_outputs = tf.nn.relu(inputs + b)\n",
    "    \n",
    "    # Return the outputs of the ReLU Layer\n",
    "    return (relu_layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a Pooling Layer and Reduce Spatial Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maxpool2by4_and_reduce_spacial_size(inputs):\n",
    "    # Create a pooling Layer\n",
    "    pooling_layer_outputs = tf.nn.max_pool(inputs, ksize=[1, 2, 2, 1], strides=[1, 2, 4, 1], padding='SAME')\n",
    "    \n",
    "    # Return the pooling Layer\n",
    "    return(pooling_layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a Fully Connected Layer and Perform Computaion: (Inputs * Weights) + Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fully_connected_layer_and_compute_dotproduct_plus_bias(inputs, output_size):\n",
    "    # Get the number of input channels from the input\n",
    "    # Inputs: the oputputs from the preceding layer or the previous operation like reshaping\n",
    "    input_size = int(inputs.get_shape()[1])\n",
    "    \n",
    "    # Intialize the weights of the filtyer of the FC layer\n",
    "    # Filter shape [in_channels, out_channels]\n",
    "    # Each weight for one filter cell\n",
    "    W = initialize_weights([input_size, output_size])\n",
    "    \n",
    "    # Intialize the bias:: each bias output channel\n",
    "    b = initialize_bias([output_size])\n",
    "    \n",
    "    # First: Perform the computation for the FC layer: dot product: inputs * W\n",
    "    # Then: Add bias to get the results: Outputs of the FC layer\n",
    "    fc_xW_plus_bias_outputs = tf.matmul(inputs, W) + b\n",
    "    \n",
    "    # Return the results: Outputs\n",
    "    return (fc_xW_plus_bias_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Placeholders for Inputs and Labels: x & y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "\n",
    "\"\"\"\n",
    "Create a placeholder for the inputs data: x\n",
    "x: a 2D array\n",
    "x: a placeholder that can hold any number of rows/records\n",
    "    Each row/record is a vector (1D Array) to hold data for one image\n",
    "    Each row/record has 784 values/elements: 1 pixel = 1 value\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder\n",
    "\n",
    "\"\"\"\n",
    "Create a placeholder for the labels of the input data: y_true\n",
    "y_true: a 2D array\n",
    "y_true: can hold any number of rows/records\n",
    "    Each row is a vector (1D Array) of 10 values that indicated a digit between 0-9\n",
    "    Each row/record: the label is stored in the one-hot format\n",
    "\"\"\"\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape=[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the Input Placeholder x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare to feed inputs into the 1st conv layer\n",
    "Reshape the input placeholder x\n",
    "From 1D arrray (vector) original input shape to a 4D-input: [batch, H, W, depth_channels]\n",
    "Depth = color channels: Grayscale = 1\n",
    "Reshaped inputs: x_image: [1, 28, 28, 1]\n",
    "\"\"\"\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Convolutional Layer, ReLU Layer, and Perform Computation: x*W + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inputs: x_image with Reshaped inputs with shape [1, 28, 28, 1]\n",
    "filter_shape: [5, 5, 1, 32]\n",
    "    Filter: 5x5 \n",
    "    Input Channels = 1\n",
    "    Output Channels = 32\n",
    "\"\"\"\n",
    "\n",
    "# Create the 1st Conv Layer \n",
    "# Then learn/extract the features, get the results (outputs): Dot product of inputs * weights\n",
    "# Return the outputs of the layer\n",
    "conv_layer_1_outputs = create_convolutional_layer_and_compute_dot_product(x_image, filter_shape=[5, 5, 1, 32])\n",
    "\n",
    "# Create the ReLU layer for the 1st Conv Layer\n",
    "# Accepts the outputs from the 1st conv layer as the inputs\n",
    "# Perform the computation at the layer: add inputs + bias\n",
    "# Return the outputs of the layer\n",
    "conv_relu_layer_1_outputs = create_relu_layer_and_compute_dotproduct_plus_b(conv_layer_1_outputs, filter_shape=[5, 5, 1, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pooling Layer and Reduce Spacial Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the 1st Pooling Layer \n",
    "Then reduce the spacial size of the input data\n",
    "Return: Outputs of the layer\n",
    "\"\"\"\n",
    "\n",
    "pooling_layer_1_outputs = create_maxpool2by4_and_reduce_spacial_size(conv_relu_layer_1_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape/Flatten Data to Make it Ready to be Fed into 1st FC Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and flatten the output of the second pooling Layer\n",
    "# Prepare to feed the output data into the 1st fully connected layer\n",
    "\n",
    "pooling_layer_1_outputs_flat = tf.reshape(pooling_layer_1_outputs, [-1, 7 * 7 * 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 1st FC Layer, ReLU Layer, and Output Data to Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the FC Layer\n",
    "Feed the flattened output of the second pooling layer as the inputs into this layer\n",
    "Then perform the computation: dot product + bias: (x * W) + b\n",
    "\n",
    "Parameters:\n",
    "    pooling_layer_2_outputs_flat\n",
    "    output_size: 1024 out channels\n",
    "\n",
    "Return: Outputs of the computation: (x * W) + b\n",
    "\"\"\"\n",
    "\n",
    "fc_layer_1_outputs = create_fully_connected_layer_and_compute_dotproduct_plus_bias(pooling_layer_1_outputs_flat, output_size=1024)\n",
    "\n",
    "# Create the ReLU layer of the 1st FC Layer\n",
    "# IMPORTANT: The activation function of this layer is also ReLU\n",
    "\n",
    "# Return: Outputs of the layer\n",
    "\n",
    "fc_relu_layer_1_outputs = tf.nn.relu(fc_layer_1_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dropout Layer and Dropout a Fraction of Outputs Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-7dac78920ceb>:12: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Declare a placeholder to hold the values of probabilitiy to keep:\n",
    "    The % of total output channels that will be kept\n",
    "    keep_prob = hold_prob = 50% -> keep 50%, drop 50%\n",
    "\"\"\"\n",
    "\n",
    "hold_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Dropout:\n",
    "# Set the outputs to 0 so that they will be ignored in the next layer\n",
    "\n",
    "fc_dropout_outputs = tf.nn.dropout(fc_relu_layer_1_outputs, keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Final FC Layer, Compute (x.W + b), and Produce Final Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the final FC Layer\n",
    "Then compute x*W + b\n",
    "Parameters:\n",
    "    fc_dropout_outputs: Outputs form the dropout layer\n",
    "    \n",
    "Return: y_pred: final predicted outputs, i.e. final classification outputs\n",
    "\"\"\"\n",
    "\n",
    "y_pred = create_fully_connected_layer_and_compute_dotproduct_plus_bias(fc_dropout_outputs, output_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Function and Calculate Softmax Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-090b5c94eb32>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define loss function: cross-entropy with logits, i.e. with the final outputs\n",
    "Calculate the softmax cross-entropy loss\n",
    "\"\"\"\n",
    "\n",
    "softmax_cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "# Compute the mean of losses\n",
    "cross_entropy_mean = tf.reduce_mean(softmax_cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Optimizer to Optimize CNN Model and Set Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an ADAM Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Trainer to Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN model trainer\n",
    "# And optimize the model by minimizing the softmax cross-entropy loss\n",
    "\n",
    "cnn_trainer = optimizer.minimize(cross_entropy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test CNN Deep Learnign Model on MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Variable Initializer to Initialize All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a variable initializer\n",
    "\n",
    "vars_initializer = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Steps Count to 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x: mnist.train= 50000 images\n",
    "Each time of training (run the whole process) = 1 step\n",
    "Each step (i.e. each round of training): Use one batch of inputs\n",
    "Batch size = 50 images -> total batches = 50000/50 = 1000 batches\n",
    "\n",
    "Steps = 3000: finish the training after 5000 runs\n",
    "\"\"\"\n",
    "\n",
    "steps = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tf.Session() to Train and Test Deep Learning CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON STEP: 0\n",
      "ACCURACY: \n",
      "0.2602\n",
      "\n",
      "\n",
      "ON STEP: 100\n",
      "ACCURACY: \n",
      "0.9084\n",
      "\n",
      "\n",
      "ON STEP: 200\n",
      "ACCURACY: \n",
      "0.9416\n",
      "\n",
      "\n",
      "ON STEP: 300\n",
      "ACCURACY: \n",
      "0.9548\n",
      "\n",
      "\n",
      "ON STEP: 400\n",
      "ACCURACY: \n",
      "0.9585\n",
      "\n",
      "\n",
      "ON STEP: 500\n",
      "ACCURACY: \n",
      "0.9604\n",
      "\n",
      "\n",
      "ON STEP: 600\n",
      "ACCURACY: \n",
      "0.9729\n",
      "\n",
      "\n",
      "ON STEP: 700\n",
      "ACCURACY: \n",
      "0.9759\n",
      "\n",
      "\n",
      "ON STEP: 800\n",
      "ACCURACY: \n",
      "0.9749\n",
      "\n",
      "\n",
      "ON STEP: 900\n",
      "ACCURACY: \n",
      "0.9791\n",
      "\n",
      "\n",
      "ON STEP: 1000\n",
      "ACCURACY: \n",
      "0.9802\n",
      "\n",
      "\n",
      "ON STEP: 1100\n",
      "ACCURACY: \n",
      "0.979\n",
      "\n",
      "\n",
      "ON STEP: 1200\n",
      "ACCURACY: \n",
      "0.982\n",
      "\n",
      "\n",
      "ON STEP: 1300\n",
      "ACCURACY: \n",
      "0.9816\n",
      "\n",
      "\n",
      "ON STEP: 1400\n",
      "ACCURACY: \n",
      "0.979\n",
      "\n",
      "\n",
      "ON STEP: 1500\n",
      "ACCURACY: \n",
      "0.9825\n",
      "\n",
      "\n",
      "ON STEP: 1600\n",
      "ACCURACY: \n",
      "0.9834\n",
      "\n",
      "\n",
      "ON STEP: 1700\n",
      "ACCURACY: \n",
      "0.9821\n",
      "\n",
      "\n",
      "ON STEP: 1800\n",
      "ACCURACY: \n",
      "0.9839\n",
      "\n",
      "\n",
      "ON STEP: 1900\n",
      "ACCURACY: \n",
      "0.9849\n",
      "\n",
      "\n",
      "ON STEP: 2000\n",
      "ACCURACY: \n",
      "0.9829\n",
      "\n",
      "\n",
      "ON STEP: 2100\n",
      "ACCURACY: \n",
      "0.9863\n",
      "\n",
      "\n",
      "ON STEP: 2200\n",
      "ACCURACY: \n",
      "0.9859\n",
      "\n",
      "\n",
      "ON STEP: 2300\n",
      "ACCURACY: \n",
      "0.9869\n",
      "\n",
      "\n",
      "ON STEP: 2400\n",
      "ACCURACY: \n",
      "0.985\n",
      "\n",
      "\n",
      "ON STEP: 2500\n",
      "ACCURACY: \n",
      "0.9861\n",
      "\n",
      "\n",
      "ON STEP: 2600\n",
      "ACCURACY: \n",
      "0.9844\n",
      "\n",
      "\n",
      "ON STEP: 2700\n",
      "ACCURACY: \n",
      "0.9844\n",
      "\n",
      "\n",
      "ON STEP: 2800\n",
      "ACCURACY: \n",
      "0.9869\n",
      "\n",
      "\n",
      "ON STEP: 2900\n",
      "ACCURACY: \n",
      "0.9873\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #First run vars_initializer to initalize all variables\n",
    "    sess.run(vars_initializer)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        # each batch: 50 images\n",
    "        batch_x, batch_y = mnist.train.next_batch(50)\n",
    "        \n",
    "        # Train the model\n",
    "        # Dropout keep_prob (% to keep): 0.5 i.e. drop 50%\n",
    "        sess.run(cnn_trainer, feed_dict={x: batch_x, y_true: batch_y, hold_prob: 0.5})\n",
    "        \n",
    "        # Test the model after each 100 steps\n",
    "        # Run this block of code after each 100 times of training\n",
    "        if i % 100 == 0:\n",
    "            print('ON STEP: {}'.format(i))\n",
    "            print('ACCURACY: ')\n",
    "            \n",
    "            # Compare to find matches of y_pred and y_true\n",
    "            matches = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "            \n",
    "            # Cast the matches from integers to tf.float32\n",
    "            # Calculate the accuracy using the mean of matches\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "            \n",
    "            # Test the model at each 100th step using test dataset\n",
    "            #Dropout: NONE because of test not training\n",
    "            test_accuracy = sess.run(acc, feed_dict = {x: mnist.test.images, \\\n",
    "                                                      y_true: mnist.test.labels, \\\n",
    "                                                      hold_prob: 1.0})\n",
    "            \n",
    "            print(test_accuracy)\n",
    "            print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
